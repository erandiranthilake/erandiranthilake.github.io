---
layout : "post"
title : "Overfitting of Model and Regularization"
author : "Erandi Ranthilake"
hasGit : true
gitProject : "https://github.com/erandiranthilake/Assigment_02"
hasLink : true
link : "https://erandiranthilake.github.io/"
linkTitle: "Home Page Link: https://erandiranthilake.github.io/"

---
<h2>Contribution :</h2>
<h3>Technical Documentation for Overfitting of Models<br>
Implementation and comparison of models based on Root Mean Squre Error and R2 Score </h3>
<hr>

<a href="https://github.com/erandiranthilake/Assigment_02">Link to Jupyter Notebook</a><br>
<hr>

<div style="text-align: justify"> 
<h2>Overfitting a model</h2>
Overfitting a model can be simply explainded as modelling a function too closely fit to a given set of data points. As the data used to train the models have a some degree of error in them, when overfitting occurs this inaccurate data infect the model leading to a substantial error. This reduces the predictive power of the model.<br><br>

<b>Reasons for Overfitting</b> are mainly using limited training data set and using complex algorythms or training techniques. We can <b>identify overfitting</b> when there is a  smaller training error and a larger testing error<br>


<h2>Demonstrating overfitting</h2>
In the developed notebook, smaller training sample is generated using y = sin(2*pi*X) + 0.1 * N<br>
Using root mean squre error, polynomial regression for different order (0, 1, 3, 9) is assessed<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/code_degree.JPG" alt="code degree"><br>
<i>Code base for models</i>
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_degree1.JPG" alt="polynomial degree"><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_degree3.JPG" alt="polynomial degree"><br>
<i>Data points plotted at different degrees</i>
<br><br>

Overfitting can be seen when the degree is 9. In order to numberically demonstrate overfitting we can calculate the training error and testing error in increasing degrees.<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/code_error.JPG" alt="code error"><br>
<i>Code base for calculating error at different degrees</i>
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_error.JPG" alt="error"><br>
<i>Training Error vs Testing Error</i>
<br><br>
<br>


<h2>How to prevent overfitting?</h2>
There are several identified ways to prevent overfitting. In this post we will dicuss preventing over fitting by increasing the amount of data points and through regularization
<br><br>

<h3>Increasing the number of Data points</h3>
As oppose to using 10 data points to train, we will use 100 Data points and a 9th order model to compare the results.
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_data100.JPG" alt="increased data">
<i>Number of data point vs fitting of the model</i>
<br><br>

We can clearely observe that the overfitting no longer exist when the number of data points is increased.
<br><br>

<h3>Regularization</h3>
We will be using a predefined regularization calculation to regularize the model and train the model. 
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_data100.JPG" alt="increased data">
<i>Number of data point vs fitting of the model</i>
<br><br>

We can clearely observe that the overfitting no longer exist when the number of data points is increased.
<br><br>

<h3>Comparison of Models based on Neuron Count</h3>
Following different models are developed to compare the base accuracies against different neuron counts.
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/neuron_count_box.JPG" alt="neuron_count_table"><br>
<i>Models and neuron counts in each layer</i>
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/neuron_count_chart.JPG" alt="neuron_count_chart"><br>
<i>Average Accuracy of each model</i>
<br><br>
Based on the above findings we can conclude that performance of the model improves with increasing number of neurons in each layer.
<br><br>

<h3>Comparison of Models based on Filter size</h3>
Following different models are developed to compare the base accuracies against different filter sizes.
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/filter_size_table.JPG" alt="filter_size_table"><br>
<i>Models and Filter sizes</i>
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/filter_size_chart.JPG" alt="filter_size_chart"><br>
<i>Average Accuracy of each model</i>
<br><br>
Based on the above findings we can conclude that performance of the model improves with decreasing filter sizes.
<br><br>

<h3>Comparison of Models based on Number of Layers</h3>
Following different models are developed to compare the base accuracies against different number of layers.
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/layers_table.JPG" alt="filter_size_table"><br>
<i>Models and Number of Layers</i>
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/layers_chart.JPG" alt="filter_size_chart"><br>
<i>Average Accuracy of each model</i>
<br><br>
Based on the above findings we can conclude that performance of the model decreases when the number of layers are considerably low or high. Performance of the model was high when the number of layers is 4.
<br><br>

<h2>Conclusion</h2>
Considering the parameters tested above we can conclude that the performance of a Convolutional Neural Network model can be improved by increasing the neuron count, decreasing the filter size and fine-tuning the number of filters accordingly.
<br><br>
Following Models are developed to test these findings and improve the performance of the base model used.
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/final_table.JPG" alt="filter_size_table"><br>
<i>Base model and enhanced model to improve performance</i>
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/final_chart.JPG" alt="filter_size_chart"><br>
<i>Average Accuracy of each model</i>
<br><br>

<h2>Challenges</h2>
Main challenge faced during the process was limitation of machine power to process increased number of layers and larger data sets. As a measure to overcome these issues, at times the models were run through limited data sets. Possible improvement concerning machine power is using GPU.
<br><br>

<hr>

<h2>References</h2>Â 
1. <a href="https://www.tensorflow.org/tutorials/images/cnn">TensorFlow Tutorial</a><br>
2. <a href="https://www.altexsoft.com/blog/image-recognition-neural-networks-use-cases/">Image Recognition with Deep Neural Networks and its Use Cases</a><br>
3. <a href="https://www.researchgate.net/profile/Wu-Xiongwei/publication/335135074_Recent_Advances_in_Deep_Learning_for_Object_Detection/links/5e27f1c8a6fdcc70a140e4ac/Recent-Advances-in-Deep-Learning-for-Object-Detection.pdf">Recent Advances in Deep Learning for Object Detection</a><br>
4. <a href="https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks">ML Practicum: Image Classification</a><br>
<br><br>

</div>
