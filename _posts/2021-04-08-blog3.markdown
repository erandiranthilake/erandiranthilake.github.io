---
layout : "post"
title : "Overfitting of Model and Regularization"
author : "Erandi Ranthilake"
hasGit : true
gitProject : "https://github.com/erandiranthilake/Assigment_02"
hasLink : true
link : "https://erandiranthilake.github.io/"
linkTitle: "Home Page Link: https://erandiranthilake.github.io/"

---
<h2>Contribution :</h2>
<h3>Technical Documentation for Overfitting of Models<br>
Studying models using Lasso and Ridge Regularization<br>
Implementation and comparison of models based on Root Mean Squre Error and R2 Score </h3>
<hr>

<a href="https://github.com/erandiranthilake/Assigment_02">Link to Jupyter Notebook</a><br>
<hr>

<div style="text-align: justify"> 
<h2>Overfitting a model</h2>
Overfitting a model can be simply explainded as modelling a function too closely fit to a given set of data points. As the data used to train the models have a some degree of error in them, when overfitting occurs this inaccurate data infect the model leading to a substantial error. This reduces the predictive power of the model.<br><br>

<b>Reasons for Overfitting</b> are mainly using limited training data set and using complex algorythms or training techniques. We can <b>identify overfitting</b> when there is a  smaller training error and a larger testing error<br>


<h2>Demonstrating overfitting</h2>
In the developed notebook, smaller training sample is generated using y = sin(2*pi*X) + 0.1 * N<br>
Using root mean squre error, polynomial regression for different order (0, 1, 3, 9) is assessed<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/code_degree.JPG" alt="code degree"><br>
<i>Code base for models</i>
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_degree1.JPG" alt="polynomial degree"><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_degree3.JPG" alt="polynomial degree"><br>
<i>Data points plotted at different degrees</i>
<br><br>

Overfitting can be seen when the degree is 9. In order to numberically demonstrate overfitting we can calculate the training error and testing error in increasing degrees.<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/code_error.JPG" alt="code error"><br>
<i>Code base for calculating error at different degrees</i>
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_error.JPG" alt="error"><br>
<i>Training Error vs Testing Error</i>
<br><br>
<br>


<h2>How to prevent overfitting?</h2>
There are several identified ways to prevent overfitting. In this post we will dicuss preventing over fitting by increasing the amount of data points and through regularization
<br><br>

<h3>Increasing the number of Data points</h3>
As oppose to using 10 data points to train, we will use 100 Data points and a 9th order model to compare the results.
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/graph_data100.JPG" alt="increased data">
<i>Number of data point vs fitting of the model</i>
<br><br>

We can clearely observe that the overfitting no longer exist when the number of data points is increased.
<br><br>

<h3>Regularization</h3>
We will be using a predefined regularization calculation to regularize the model and train the model. THe behaviour of the model is observed with changing alpha values.
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/regu_1.JPG" alt="regularization 1">
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/regu_2.JPG" alt="regularization 2">
<i>Regularization: Fitting of the model with changing alpha values</i>
<br><br>

To analyse these data more in depth, we can see how Root mean squre error changes with different alpha values. In the folliwing figures you can see how the training and test error changes with Lasso Regularization and Ridge Regularization, two commonly used regularization techniques for regression models.
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/lasso_regu.JPG" alt="Lasso regularization">
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/ridge_regu.JPG" alt="Ridge regularization">
<br><br>

<h2>Root Mean Squre Error and R2 score to assess overfitting</h2>
Another important measure that we can use in order to assess over fitting is r2 scores. We can see in polynomial regression model, when the degree is increasing, thus increasing the complexity of the model, the error increases and R2 score gradually decreases. 
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/r2_score.JPG" alt="r2 score"><br>
<i>R2 score and ERMS with increasing degree</i>
<br><br>

<h2>Challenges</h2>
Main challenge faced during assessing these models was to accurate assessment of the complexity of the training technique as well as the amount of data needed to build a successful predictive model. Graph tools used during the process helped to visualize the results and assess these parameters with more accuracy.
<br><br>

<hr>

<h2>References</h2>Â 
1. <a href="https://www.investopedia.com/terms/o/overfitting.asp">Overfitting</a><br>
2. <a href="https://towardsdatascience.com/polynomial-regression-bbe8b9d97491">Polynomial Regression</a><br>
3. <a href="https://www.pluralsight.com/guides/linear-lasso-ridge-regression-scikit-learn">Linear, Lasso, and Ridge Regression with scikit-learn</a><br>
<br><br>

</div>
