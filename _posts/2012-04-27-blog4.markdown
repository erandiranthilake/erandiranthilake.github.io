---
layout : "post"
title : "Text Analysis Naive Bayes Classification"
author : "Erandi Ranthilake"
hasGit : false
gitProject : "https://github.com/erandiranthilake/Assigment_02"
hasLink : true
link : "https://erandiranthilake.github.io/"
linkTitle: "Home Page Link: https://erandiranthilake.github.io/"

---
<h2>Contribution :</h2>
<h3>Technical Documentation for Naive Bayes Classification<br>
Experiments on improving Naive bayes classification Model<br>
Experiments on Improving Text Analysis<br>
 </h3>
<hr>

<a href="https://www.kaggle.com/erandiranthilake/naive-bayes">Link to Kaggle Notebook</a><br>
<hr>

<div style="text-align: justify"> 
<h2>Naive Bayes Classification</h2>
Naïve Bayes classification is a method used to calculate the probability of a outcome, given certain features, which can be depicted as P(L|features).  This is a very strong data minning technique for predictive models. The base theory behind this classifier is the Bayes's Theorem, which describes the relationship of conditional probabilities of statistical computations in the following equation.<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/code_degree.JPG" alt="code degree"><br>
<i>Code base for models</i>
<br><br>

<b>Why is it called 'Naive'?</b><br>
The Naïve Bayes is called 'Naïve' as it makes the assumption that we are sing independent input variable in the calculation. This is an unrealistic assumption. However, the technique is highly effective and solves many complex problems, which outweighs the effect of the unrealistic assumption.<br>


<h2>Sentiment analysis of test using Naïve Bayes classification</h2>
Text analysis can be defined as parsing unstructured textual data, in order to extract facts from it that can be processed by machine learning models. The challenge of analysing text is the ambiguity of languages. Given a certain text, a machine can generate several valid interpretations. Sentiment Analysis of text is basically identifying if a positive or a negative sentiment is attached to the text. In this exercise, we will try to develop a Naïve Bayes classification model to Identify if a given review is negative or positive.<br><br>

<b>Data</b><br>
We will use the <a href="https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set">Sentiment Labelled Sentences Data Set from Kaggle.com</a> for this exercise. Our goal is predicting the sentiment. <br>
First, we will load the data into a dataframe.<br><br>


<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/load_data.JPGG" alt="Load Data"><br>
<i>Load Data into a Dataframe</i>
<br><br>

Then we will divide the data into Train, Dev and Test data sets.<br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/divide_data.JPG" alt="divide Data"><br>
<i>Divide Data into a Dataframe</i>
<br><br>

<b>Building Vocabulary</b><br>
Next step is building a vocabulary for the train data. This will in the format of a dictionary where we will calculate the number of documents with the positive comments, nnumber of documents with the positive comments and the number of all the documents are recorded for each word. <br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/vocab_code.JPG" alt="build vocabulary"><br>
<i>Build a vocabulary for each word</i>
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/vocab_result.JPG" alt="vocab result"><br>
<i> developed Vocabulary</i>
<br><br>


<b>Develop Probability Calculations</b><br>
Using the developed Vocabulary in the previous step, we can develop methods to calculate certain probabilities that will be used in the sentiment analysis of our texts.<br><br>
1. Probability of the occurrence<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/poo.JPG" alt="Probability of the occurrence"><br>
<i>Probability of the occurrence</i>
<br><br>

2. Conditional probability based on the sentiment<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/cond_prob.JPG" alt="Conditional probability"><br>
<i> Conditional probability based on the sentiment</i>
<br><br>

Following is the outcome of these methods when we calculate above probabilities for the word "the"<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/the_prob.JPG" alt="calcualte probability"><br>
<i> Calculated Probabilities for teh word "the"</i>
<br><br>


<h3>Regularization</h3>
We will be using a predefined regularization calculation to regularize the model and train the model. THe behaviour of the model is observed with changing alpha values.
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/regu_1.JPG" alt="regularization 1">
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/regu_2.JPG" alt="regularization 2">
<i>Regularization: Fitting of the model with changing alpha values</i>
<br><br>

To analyse these data more in depth, we can see how Root mean squre error changes with different alpha values. In the folliwing figures you can see how the training and test error changes with Lasso Regularization and Ridge Regularization, two commonly used regularization techniques for regression models.
<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/lasso_regu.JPG" alt="Lasso regularization">
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/ridge_regu.JPG" alt="Ridge regularization">
<br><br>

<h2>Root Mean Squre Error and R2 score to assess overfitting</h2>
Another important measure that we can use in order to assess over fitting is r2 scores. We can see in polynomial regression model, when the degree is increasing, thus increasing the complexity of the model, the error increases and R2 score gradually decreases. 
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/r2_score.JPG" alt="r2 score"><br>
<i>R2 score and ERMS with increasing degree</i>
<br><br>

<h2>Challenges</h2>
Main challenge faced during assessing these models was to accurate assessment of the complexity of the training technique as well as the amount of data needed to build a successful predictive model. Graph tools used during the process helped to visualize the results and assess these parameters with more accuracy.
<br><br>

<hr>

<h2>References</h2> 
1. <a href="https://www.investopedia.com/terms/o/overfitting.asp">Overfitting</a><br>
2. <a href="https://towardsdatascience.com/polynomial-regression-bbe8b9d97491">Polynomial Regression</a><br>
3. <a href="https://www.pluralsight.com/guides/linear-lasso-ridge-regression-scikit-learn">Linear, Lasso, and Ridge Regression with scikit-learn</a><br>
<br><br>

</div>
