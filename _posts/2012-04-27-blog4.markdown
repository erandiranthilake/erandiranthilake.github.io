---
layout : "post"
title : "Text Analysis Naive Bayes Classification"
author : "Erandi Ranthilake"
hasGit : false
gitProject : "https://github.com/erandiranthilake/Assigment_02"
hasLink : true
link : "https://erandiranthilake.github.io/"
linkTitle: "Home Page Link: https://erandiranthilake.github.io/"

---
<h2>Contribution :</h2>
<h3>Technical Documentation for Naive Bayes Classification<br>
Experiments on improving Naive bayes classification Model<br>
Experiments on Improving Text Analysis<br>
 </h3>
<hr>

<a href="https://www.kaggle.com/erandiranthilake/naive-bayes">Link to Kaggle Notebook</a><br>
<hr>

<div style="text-align: justify"> 
<h2>Naive Bayes Classification</h2>
Naïve Bayes classification is a method used to calculate the probability of a outcome, given certain features, which can be depicted as P(L|features).  This is a very strong data minning technique for predictive models. The base theory behind this classifier is the Bayes's Theorem, which describes the relationship of conditional probabilities of statistical computations in the following equation.<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/code_degree.JPG" alt="code degree"><br>
<i>Code base for models</i>
<br><br>

<b>Why is it called 'Naive'?</b><br>
The Naïve Bayes is called 'Naïve' as it makes the assumption that we are sing independent input variable in the calculation. This is an unrealistic assumption. However, the technique is highly effective and solves many complex problems, which outweighs the effect of the unrealistic assumption.<br>


<h2>Sentiment analysis of test using Naïve Bayes classification</h2>
Text analysis can be defined as parsing unstructured textual data, in order to extract facts from it that can be processed by machine learning models. The challenge of analysing text is the ambiguity of languages. Given a certain text, a machine can generate several valid interpretations. Sentiment Analysis of text is basically identifying if a positive or a negative sentiment is attached to the text. In this exercise, we will try to develop a Naïve Bayes classification model to Identify if a given review is negative or positive.<br><br>

<b>Data</b><br>
We will use the <a href="https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set">Sentiment Labelled Sentences Data Set from Kaggle.com</a> for this exercise. Our goal is predicting the sentiment. <br>
First, we will load the data into a dataframe.<br><br>


<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/load_data.JPGG" alt="Load Data"><br>
<i>Load Data into a Dataframe</i>
<br><br>

Then we will divide the data into Train, Dev and Test data sets.<br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/divide_data.JPG" alt="divide Data"><br>
<i>Divide Data into a Dataframe</i>
<br><br>

<b>Building Vocabulary</b><br>
Next step is building a vocabulary for the train data. This will in the format of a dictionary where we will calculate the number of documents with the positive comments, nnumber of documents with the positive comments and the number of all the documents are recorded for each word. <br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/vocab_code.JPG" alt="build vocabulary"><br>
<i>Build a vocabulary for each word</i>
<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/vocab_result.JPG" alt="vocab result"><br>
<i> developed Vocabulary</i>
<br><br>


<b>Develop Probability Calculations</b><br>
Using the developed Vocabulary in the previous step, we can develop methods to calculate certain probabilities that will be used in the sentiment analysis of our texts.<br><br>
1. Probability of the occurrence<br><br>

<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/poo.JPG" alt="Probability of the occurrence"><br>
<i>Probability of the occurrence</i>
<br><br>

2. Conditional probability based on the sentiment<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/cond_prob.JPG" alt="Conditional probability"><br>
<i> Conditional probability based on the sentiment</i>
<br><br>

Following is the outcome of these methods when we calculate above probabilities for the word "the"<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/the_prob.JPG" alt="calcualte probability"><br>
<i> Calculated Probabilities for the word "the"</i>
<br><br>


<h3>Predicting if the review is 'Positive' or 'Negative'</h3>
Using the above calculations now we can predict if a given text is 'Positive' or 'Negative' in sentiment.
<br><br>

<b>Methods</b><br>
The predict method is used to predict the sentiment for a given group of words and the caculate_accuracy method<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/predict.JPG" alt="Predict Setiment"><br>
<i> Methods to predict sentiment and caculate the accuracy</i>
<br><br>

In addition, we will use the Five-fold cross validation to extend our accuracy calculations<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/five_fold.JPG" alt="five_fold_cross validation_"><br>
<i> Five fold cross validation of the data set</i>
<br><br>

Now we can calcuate the Accuracy of the prediction using the dev dataset and the five fold cross validation.<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/results_dev.JPG" alt="results_dev"><br>
<i> Calculation results using dev data set</i>
<br><br>

<h2>Improving the Model using Smoothing</h2>
If a given feature and the outcome never occur together in the training dataset, the probability calculation for all the features will be zero. This will lead to an incorrect prediction as the other features that occurred with the outcome could actually have an impact on the predict. In order to prevent this we introduce Smoothing. <br><br>

In smoothing when we calculate the conditional probability, we add 1 to the numerator and number of unique occurrences of the features to the denominator. If we are developing a binary classification model we can simply add 2 to the denominator.<br><br>

Following is the modified conditional probability calculation to implement smoothing<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/cond_prob_smoothing.JPG" alt="cond_prob_smoothing.JPG"><br>
<i> Modified conditional probability calculation to implement smoothing</i>
<br><br>


we can derive Top 10 words that predicts positive and negative class and use that to improve our model as well. <br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/top_ten.JPG" alt="top_ten.JPG"><br>
<i> Top 10 words that predicts positive and negative class</i>
<br><br>

<b>Results after applying Smoothing</b><br>
Now we can calculate the accuracy and conduct Five-fold cross validation to the data set and compare if the technique has improved the model.<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/results_smoothing.JPG" alt="results_smoothing"><br>
<i> Calculation results using dev data set after applying smoothing</i>
<br><br>

Finally, we can calculate the accuracy using the test data set.<br><br>
<img src="https://raw.githubusercontent.com/erandiranthilake/erandiranthilake.github.io/gh-pages/images/test_data.JPG" alt="test_data.JPGg"><br>
<i> Calculation results using test data set after applying smoothing</i>
<br><br>

<h2>Conclusion</h2>
Sentiment analysis of text is an important topic in the Machine Learning and data mining word as it has many uses in the real word. Naïve Bayes classification, a simple yet powerful technique can be used to analyze text.<br><br>
We can see a significant improvement in the prediction accuracies when we use the smoothing technique to improve the model. The accuracy of the dev data set improved from 0.48 to 0.68, where as the average of the five-fold cross validation improved from 0.494 to 0.599.<br>
Therefore, we can recommend using smoothing as a technique to improve such models.
<br><br>

<h2>Challenges</h2>
Main challenge in the exercise was the time it takes for the calculation. Since the classification model include many probability calculations, the calcualtion time is considerably large.
<br><br>

<hr>

<h2>References</h2> 
1. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html">In Depth: Naive Bayes Classification</a><br>
2. <a href="https://www.sciencedirect.com/topics/mathematics/naive-bayes#:~:text=Naive%20Bayes%20is%20called%20naive,large%20range%20of%20complex%20problems.">Naïve Bayes</a><br>
3. <a href="https://www.ontotext.com/knowledgehub/fundamentals/text-analysis/">What is Text Analysis?</a><br>
4. <a href="https://courses.cs.washington.edu/courses/cse446/20wi/Section7/naive-bayes.pdf">Naive Bayes Classifier</a><br>
<br><br>

</div>
